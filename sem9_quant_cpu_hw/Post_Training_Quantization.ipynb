{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ypQKMC0QIjA",
        "outputId": "899edec5-9a3b-4773-a151-f116ade43f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                                    2.8.0+cu126\n",
            "torchao                                  0.10.0\n",
            "torchaudio                               2.8.0+cu126\n",
            "torchdata                                0.11.0\n",
            "torchsummary                             1.5.1\n",
            "torchtune                                0.6.1\n",
            "torchvision                              0.23.0+cu126\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhEMHX1V4IwM"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoMW3Ruptax1"
      },
      "source": [
        "*   https://pytorch.org/blog/quantization-in-practice/\n",
        "*   https://pytorch.org/docs/stable/quantization.html\n",
        "*   https://pytorch.org/docs/stable/quantization-support.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqyoRrl7A0y"
      },
      "source": [
        "### Mapping function and Quantization Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbgzwD7M1KW8"
      },
      "outputs": [],
      "source": [
        "# r - float tensor, r' - int tensor\n",
        "# S [scaling factor] = (beta - alpha) / (beta_q - alpha_q)\n",
        "# Z [zero point] =  -(alpha / S - alpha_q)\n",
        "\n",
        "def quantize(float_tensor, scale, z):\n",
        "  # Q(r) = round(r/S + Z)\n",
        "  return torch.round(float_tensor / scale + z)\n",
        "\n",
        "def dequantize(int_tensor, scale, z):\n",
        "  # r' = (Q(r) - Z) * S\n",
        "  return (int_tensor - z) * scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBAf6XZi1KXs",
        "outputId": "45636d95-6d73-460a-ad20-0e06484a8510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3662,  0.1390,  0.3182,  0.4773],\n",
            "        [ 0.1317,  0.3808, -0.4732, -1.9495],\n",
            "        [ 1.0165,  0.3677,  0.2789,  1.4896]])\n"
          ]
        }
      ],
      "source": [
        "from torch.ao.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
        "\n",
        "C, L = 3, 4\n",
        "normal = torch.distributions.normal.Normal(0,1)\n",
        "inputs = normal.sample((C, L))\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPanD_eQ4TZl",
        "outputId": "ed207dd6-570f-42b9-e8ec-afe99c7d8f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MinMaxObserver (tensor([0.0135]), tensor([145], dtype=torch.int32))\n",
            "MovingAverageMinMaxObserver (tensor([0.0135]), tensor([145], dtype=torch.int32))\n",
            "HistogramObserver (tensor([0.0135]), tensor([145], dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "observers = [MinMaxObserver(), MovingAverageMinMaxObserver(), HistogramObserver()]\n",
        "for obs in observers:\n",
        "  obs(inputs)\n",
        "  print(obs.__class__.__name__, obs.calculate_qparams())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZf3OgNQ49Aj",
        "outputId": "36eda241-9b88-4a4b-846c-a5ecb0f1ecb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0021, 0.0042, 0.0055, 0.0052],\n",
              "        [0.0032, 0.0032, 0.0012, 0.0060],\n",
              "        [0.0050, 0.0036, 0.0043, 0.0060]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scale, z = observers[0].calculate_qparams()\n",
        "reconstruction_error = torch.abs(dequantize(quantize(inputs, scale, z), scale, z) - inputs)\n",
        "reconstruction_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb5qUkHP8sUE"
      },
      "source": [
        "### Affine and Symmetric Quantization Schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8cv_y6f8tT3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_symmetric_range(x):\n",
        "  beta = torch.max(x.max(), x.min().abs())\n",
        "  return -beta.item(), beta.item()\n",
        "\n",
        "def get_affine_range(x):\n",
        "  return x.min().item(), x.max().item()\n",
        "\n",
        "def plot(plt, data, scheme):\n",
        "  boundaries = get_affine_range(data) if scheme == 'affine' else get_symmetric_range(data)\n",
        "  a, _, _ = plt.hist(data, density=True, bins=100)\n",
        "  ymin, ymax = np.quantile(a[a>0], [0.25, 0.95])\n",
        "  plt.vlines(x=boundaries, ls='--', colors='purple', ymin=ymin, ymax=ymax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "NTMv0kcB8128",
        "outputId": "e6a3d4a5-563a-4bea-a881-51a9db06ea02"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "act =  torch.distributions.pareto.Pareto(1, 10).sample((1, 1024))\n",
        "weights = torch.distributions.normal.Normal(0, 0.12).sample((3, 64, 7, 7)).flatten()\n",
        "\n",
        "fig, axs = plt.subplots(2,2)\n",
        "plot(axs[0, 0], act, 'affine')\n",
        "axs[0, 0].set_title(\"Activation, Affine-Quantized\")\n",
        "\n",
        "plot(axs[0, 1], act, 'symmetric')\n",
        "axs[0, 1].set_title(\"Activation, Symmetric-Quantized\")\n",
        "\n",
        "plot(axs[1, 0], weights, 'affine')\n",
        "axs[1, 0].set_title(\"Weights, Affine-Quantized\")\n",
        "\n",
        "plot(axs[1, 1], weights, 'symmetric')\n",
        "axs[1, 1].set_title(\"Weights, Symmetric-Quantized\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bej5FWvR-UDK",
        "outputId": "4545bf0c-6b49-4667-bb55-fb6b3c16405b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qscheme: torch.per_tensor_affine | (tensor([0.0135]), tensor([145], dtype=torch.int32))\n",
            "Qscheme: torch.per_tensor_symmetric | (tensor([0.0153]), tensor([128]))\n"
          ]
        }
      ],
      "source": [
        "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
        "  obs = MovingAverageMinMaxObserver(qscheme=qscheme)\n",
        "  obs(inputs)\n",
        "  print(f\"Qscheme: {qscheme} | {obs.calculate_qparams()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc4Zd5M99fTo"
      },
      "source": [
        "### Per-Tensor and Per-Channel Quantization Schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dltyo4q9l-4",
        "outputId": "a0020a37-635e-408d-aa2d-3c8b8af866fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0.0019, 0.0091, 0.0058]), tensor([  0, 213,   0], dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "from torch.ao.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
        "\n",
        "obs = MovingAveragePerChannelMinMaxObserver(ch_axis = 0)  # calculate qparams for all `C` channels separately\n",
        "obs(inputs)\n",
        "print(obs.calculate_qparams())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOd8W97_AAMW"
      },
      "source": [
        "For weights quantization, symmetric-per-channel quantization provides better accuracies; per-tensor quantization performs poorly, possibly due to high variance in conv weights across channels from batchnorm folding.\n",
        "[https://arxiv.org/abs/2004.09602]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvWTOGgv9gmI"
      },
      "source": [
        "https://github.com/pytorch/pytorch/blob/748d9d24940cd17938df963456c90fa1a13f3932/torch/ao/quantization/observer.py#L258"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiBMYYJMBC0A"
      },
      "source": [
        "### Backend Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OgF7U86BDw7"
      },
      "outputs": [],
      "source": [
        "backend = 'fbgemm' #'fbgemm' if x86 else 'qnnpack' intel - oneDNN\n",
        "qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
        "torch.backends.quantized.engine = backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atxxYA28Bcvz"
      },
      "source": [
        "GPUs - via TensorRT and cuDNN\n",
        "https://pytorch.org/docs/stable/quantization.html#note-for-native-cpu-backends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk-uxMCWBpRl"
      },
      "source": [
        "### QConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3xygBpkBqKW",
        "outputId": "8953ea54-c556-4fef-8a88-cae8585534ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_affine){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.qint8){})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_qconfig = torch.ao.quantization.QConfig(\n",
        "  activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine),\n",
        "  weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8))\n",
        "\n",
        "my_qconfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvsq4t68Kwj7",
        "outputId": "497c7aef-f6d4-464c-ed63-a9a24b63059a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.per_channel_affine"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mh00mEaV4wy"
      },
      "source": [
        "### Eager Mode v/s FX Graph Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLkEva02V_Co"
      },
      "source": [
        "*    Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.\n",
        "*    FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently its a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWjnQot3Wzg-"
      },
      "source": [
        "FX Graph Mode automatically fuses eligible modules, inserts Quant/DeQuant stubs, calibrates the model and returns a quantized module - all in two method calls - but only for networks that are symbolic traceable. The examples below contain the calls using Eager Mode and FX Graph Mode for comparison.\n",
        "https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el4bJKgIV6y7"
      },
      "outputs": [],
      "source": [
        "def f(a, b):\n",
        "    if b == True:\n",
        "        return a\n",
        "    else:\n",
        "        return a * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "PkVCexZefVIX",
        "outputId": "13f97a27-4796-45a5-bf74-2609faa0c745"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "b has been specialized to have value False but got another value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1040574657.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_wrapped\u001b[0m  \u001b[0;31m# type: ignore[method-assign]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: B904\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<eval_with_key>.0 from /tmp/ipython-input-2855332725.py:1 in f\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a, b_1)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mb_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_assert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b has been specialized to have value False but got another value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_assert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   2172\u001b[0m             \u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m         )\n\u001b[0;32m-> 2174\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: b has been specialized to have value False but got another value"
          ]
        }
      ],
      "source": [
        "f = torch.fx.symbolic_trace(f, concrete_args={'b': False})\n",
        "assert f(3, True) == 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Temh_oTBfjE8"
      },
      "source": [
        "https://pytorch.org/docs/stable/fx.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfxsmoBtXOgb"
      },
      "source": [
        "### Post-Training Dynamic/Weight-only Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APgH-cqYdx6O"
      },
      "source": [
        "Here the models weights are pre-quantized; the activations are quantized on-the-fly (dynamic) during inference. The simplest of all approaches, it has a one line API call in torch.quantization.quantize_dynamic. Currently only Linear and Recurrent (LSTM, GRU, RNN) layers are supported for dynamic quantization\n",
        "\n",
        "\n",
        "\n",
        "*   Can result in higher accuracies since the clipping range is exactly calibrated for each input\n",
        "*   Calibrating and quantizing the activations at each layer during runtime can add to the compute overhead\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqIHv8efWt2l",
        "outputId": "421b9f02-29ee-467c-fc16-21a59e4d4dc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(2, 64, kernel_size=(8,), stride=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=16, out_features=10, bias=True)\n",
              "  (3): LSTM(10, 10)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "# toy model\n",
        "m = nn.Sequential(\n",
        "  nn.Conv2d(2, 64, (8,)),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(16, 10),\n",
        "  nn.LSTM(10, 10))\n",
        "\n",
        "m.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jhhTCrvXYv4",
        "outputId": "d486e6f4-de87-462e-a94d-6b7a43470eee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2181302342.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_dynamic(\n"
          ]
        }
      ],
      "source": [
        "## EAGER MODE\n",
        "from torch.ao.quantization import quantize_dynamic\n",
        "\n",
        "model_quantized = quantize_dynamic(\n",
        "    model=m, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC5aJPPgXaO_",
        "outputId": "307c973a-3893-41fd-f957-7f8176c47393"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-15622409.py:6: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/tmp/ipython-input-15622409.py:7: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/nn/quantized/reference/modules/rnn.py:461: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/nn/quantized/reference/modules/rnn.py:467: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(\n"
          ]
        }
      ],
      "source": [
        "## FX MODE\n",
        "from torch.ao.quantization import quantize_fx\n",
        "\n",
        "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "qconfig_dict = {\"\": torch.ao.quantization.default_dynamic_qconfig}  # An empty key denotes the default applied to all modules\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwvpek4HMxer",
        "outputId": "ed7acdb3-581e-48eb-d267-b3329df2a630"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.quint8, quant_min=0, quant_max=255, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.ao.quantization.default_dynamic_qconfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Dd8gxLaNmy"
      },
      "source": [
        "### Post-Training Static Quantization (PTQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCZcZdAIdTzg"
      },
      "source": [
        "PTQ also pre-quantizes model weights but instead of calibrating activations on-the-fly, the clipping range is pre-calibrated and fixed (static) using validation data. Activations stay in quantized precision between operations during inference. About 100 mini-batches of representative data are sufficient to calibrate the observers\n",
        "\n",
        "*   Static quantization has faster inference than dynamic quantization because it eliminates the float<->int conversion costs between layers\n",
        "*   Static quantized models may need regular re-calibration to stay robust against distribution-drift\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v3vGj4WaQYV"
      },
      "outputs": [],
      "source": [
        "# Static quantization of a model consists of the following steps:\n",
        "\n",
        "#     Fuse modules\n",
        "#     Insert Quant/DeQuant Stubs\n",
        "#     Prepare the fused module (insert observers before and after layers)\n",
        "#     Calibrate the prepared module (pass it representative data)\n",
        "#     Convert the calibrated module (replace with quantized version)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import copy\n",
        "\n",
        "backend = \"fbgemm\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
        "\n",
        "model = nn.Sequential(\n",
        "     nn.Conv2d(2, 64, 3),\n",
        "     nn.ReLU(),\n",
        "     nn.Conv2d(64, 128, 3),\n",
        "     nn.ReLU()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "UaZu_-p7OMgf",
        "outputId": "97e7167b-3973-4b5d-c119-29e38d860d99"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'Parameter' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3628335316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Parameter' object is not callable"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgNoxdlneyuM",
        "outputId": "ec0276b7-8422-404a-a376-326f3c3dcab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-213743822.py:18: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.ao.quantization.prepare(m, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-213743822.py:29: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.ao.quantization.convert(m, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "## EAGER MODE\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "\n",
        "\"\"\"Fuse\n",
        "- Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules\n",
        "\"\"\"\n",
        "torch.ao.quantization.fuse_modules(m, ['0','1'], inplace=True) # fuse first Conv-ReLU pair\n",
        "torch.ao.quantization.fuse_modules(m, ['2','3'], inplace=True) # fuse second Conv-ReLU pair\n",
        "\n",
        "\"\"\"Insert stubs\"\"\"\n",
        "m = nn.Sequential(torch.ao.quantization.QuantStub(),\n",
        "                  *m,\n",
        "                  torch.ao.quantization.DeQuantStub())\n",
        "\n",
        "\"\"\"Prepare\"\"\"\n",
        "m.qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
        "torch.ao.quantization.prepare(m, inplace=True)\n",
        "\n",
        "\"\"\"Calibrate\n",
        "- This example uses random data for convenience. Use representative (validation) data instead.\n",
        "\"\"\"\n",
        "with torch.inference_mode():\n",
        "  for _ in range(10):\n",
        "    x = torch.rand(1, 2, 28, 28)\n",
        "    m(x)\n",
        "\n",
        "\"\"\"Convert\"\"\"\n",
        "torch.ao.quantization.convert(m, inplace=True)\n",
        "\n",
        "\"\"\"Check\"\"\"\n",
        "print(m[1].weight().element_size()) # 1 byte instead of 4 bytes for FP32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmyPkO1Bi-Tw",
        "outputId": "96246408-72a0-43d3-9c58-142aac5f0c9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 0.2192, -0.1191,  0.0293],\n",
              "          [-0.0138, -0.1536,  0.1191],\n",
              "          [ 0.0811,  0.0604,  0.2175]],\n",
              "\n",
              "         [[ 0.0069, -0.0863, -0.0794],\n",
              "          [-0.1554, -0.0794,  0.1433],\n",
              "          [ 0.0190,  0.1364,  0.1554]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0805, -0.1216,  0.1162],\n",
              "          [-0.0662, -0.0143,  0.0680],\n",
              "          [ 0.1216,  0.2146,  0.1413]],\n",
              "\n",
              "         [[-0.2289,  0.1967, -0.0733],\n",
              "          [-0.0107,  0.1592, -0.0143],\n",
              "          [-0.0036,  0.0912,  0.1449]]],\n",
              "\n",
              "\n",
              "        [[[-0.0942, -0.1309,  0.1692],\n",
              "          [ 0.0995,  0.1012, -0.1099],\n",
              "          [-0.0262,  0.0192,  0.0087]],\n",
              "\n",
              "         [[ 0.1064,  0.0017,  0.1256],\n",
              "          [-0.2094,  0.1064, -0.0628],\n",
              "          [ 0.2216,  0.1815,  0.2216]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[-0.0036,  0.0874,  0.0473],\n",
              "          [ 0.0874,  0.0983, -0.2203],\n",
              "          [ 0.2313,  0.1074,  0.1475]],\n",
              "\n",
              "         [[-0.0437,  0.0874, -0.2276],\n",
              "          [ 0.1584, -0.1348, -0.1912],\n",
              "          [ 0.1584, -0.0091,  0.1967]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0632,  0.2023,  0.1246],\n",
              "          [-0.1698, -0.1608, -0.1264],\n",
              "          [ 0.0470, -0.0939,  0.2258]],\n",
              "\n",
              "         [[-0.1373,  0.0560,  0.1463],\n",
              "          [-0.0885,  0.2294, -0.0506],\n",
              "          [-0.1662,  0.1228,  0.1572]]],\n",
              "\n",
              "\n",
              "        [[[-0.0425, -0.0630,  0.1496],\n",
              "          [ 0.0047, -0.1701, -0.1748],\n",
              "          [ 0.0567, -0.0457, -0.0457]],\n",
              "\n",
              "         [[-0.0079, -0.1638,  0.1590],\n",
              "          [ 0.0929,  0.2000,  0.0504],\n",
              "          [ 0.1228, -0.2015, -0.1338]]]], size=(64, 2, 3, 3),\n",
              "       dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
              "       scale=tensor([0.0017, 0.0018, 0.0017, 0.0017, 0.0018, 0.0015, 0.0018, 0.0018, 0.0018,\n",
              "        0.0017, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0015, 0.0018,\n",
              "        0.0018, 0.0018, 0.0017, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018,\n",
              "        0.0018, 0.0018, 0.0017, 0.0018, 0.0018, 0.0017, 0.0016, 0.0018, 0.0018,\n",
              "        0.0017, 0.0017, 0.0018, 0.0018, 0.0018, 0.0018, 0.0016, 0.0018, 0.0018,\n",
              "        0.0017, 0.0016, 0.0018, 0.0018, 0.0017, 0.0016, 0.0018, 0.0018, 0.0018,\n",
              "        0.0018, 0.0018, 0.0016, 0.0014, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018,\n",
              "        0.0016], dtype=torch.float64),\n",
              "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "       axis=0)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m[1].weight()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8a-ihuEe13L",
        "outputId": "5b608eef-e712-456c-f362-a0eeb5b18d72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-750729956.py:10: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
            "/tmp/ipython-input-750729956.py:19: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n"
          ]
        }
      ],
      "source": [
        "## FX GRAPH\n",
        "from torch.ao.quantization import quantize_fx\n",
        "\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "qconfig_dict = {\"\": torch.ao.quantization.get_default_qconfig(backend)}\n",
        "\n",
        "# Prepare\n",
        "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
        "\n",
        "# Calibrate - Use representative (validation) data.\n",
        "with torch.inference_mode():\n",
        "  for _ in range(10):\n",
        "    x = torch.rand(1, 2, 28, 28)\n",
        "    model_prepared(x)\n",
        "\n",
        "# quantize\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT-qggJdacLr"
      },
      "source": [
        "### SENSITIVITY ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD294D_Tac3K",
        "outputId": "35a8faa1-0d31-43e9-f1f1-ab8da1f0e6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Only quantizing layer:  \n",
            "Only quantizing layer:  0\n",
            "Only quantizing layer:  1\n",
            "Only quantizing layer:  2\n",
            "Only quantizing layer:  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1147990886.py:11: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/tmp/ipython-input-1147990886.py:13: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# ONE-AT-A-TIME SENSITIVITY ANALYSIS\n",
        "\n",
        "for quantized_layer, _ in model.named_modules():\n",
        "  print(\"Only quantizing layer: \", quantized_layer)\n",
        "\n",
        "  # The module_name key allows module-specific qconfigs.\n",
        "  qconfig_dict = {\"\": None,\n",
        "  \"module_name\":[(quantized_layer, torch.quantization.get_default_qconfig(backend))]}\n",
        "\n",
        "  example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, example_inputs)\n",
        "  # calibrate\n",
        "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
        "  # evaluate(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dXAIoKwbCFl"
      },
      "source": [
        " Numeric Suite - https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7FfZtQdbV90"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import torch.ao.quantization\n",
        "import torch.ao.ns._numeric_suite as ns\n",
        "from torch.ao.quantization import (\n",
        "    default_eval_fn,\n",
        "    default_qconfig,\n",
        "    quantize,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm-ESg6obWWp",
        "outputId": "f840fb06-be55-4980-c4c4-11abf7ea0a1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 44.7M/44.7M [00:00<00:00, 189MB/s]\n",
            "/tmp/ipython-input-409614095.py:7: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  qmodel = quantize(float_model, default_eval_fn, [[img_data]], inplace=False)\n"
          ]
        }
      ],
      "source": [
        "float_model = torchvision.models.quantization.resnet18(pretrained=True, quantize=False)\n",
        "float_model.to('cpu')\n",
        "float_model.eval()\n",
        "float_model.fuse_model()\n",
        "float_model.qconfig = torch.quantization.default_qconfig\n",
        "img_data = (torch.rand(2, 3, 10, 10, dtype=torch.float), torch.randint(0, 1, (2,), dtype=torch.long))\n",
        "qmodel = quantize(float_model, default_eval_fn, [[img_data]], inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKZEOpfPbMNu",
        "outputId": "f5cb9320-1905-4a8d-b7a4-7c472fc1ad4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.weight tensor(31.6638)\n",
            "layer1.0.conv1.weight tensor(30.6450)\n",
            "layer1.0.conv2.weight tensor(31.1528)\n",
            "layer1.1.conv1.weight tensor(32.1438)\n",
            "layer1.1.conv2.weight tensor(31.2477)\n",
            "layer2.0.conv1.weight tensor(30.9890)\n",
            "layer2.0.conv2.weight tensor(28.8233)\n",
            "layer2.0.downsample.0.weight tensor(31.5558)\n",
            "layer2.1.conv1.weight tensor(30.7668)\n",
            "layer2.1.conv2.weight tensor(28.4516)\n",
            "layer3.0.conv1.weight tensor(30.9247)\n",
            "layer3.0.conv2.weight tensor(26.6841)\n",
            "layer3.0.downsample.0.weight tensor(28.7825)\n",
            "layer3.1.conv1.weight tensor(28.9707)\n",
            "layer3.1.conv2.weight tensor(25.6784)\n",
            "layer4.0.conv1.weight tensor(26.8495)\n",
            "layer4.0.conv2.weight tensor(25.8394)\n",
            "layer4.0.downsample.0.weight tensor(28.6355)\n",
            "layer4.1.conv1.weight tensor(26.8758)\n",
            "layer4.1.conv2.weight tensor(28.4319)\n",
            "fc._packed_params._packed_params tensor(32.6505)\n",
            "---\n",
            "conv1.stats tensor(37.5421, grad_fn=<MulBackward0>)\n",
            "layer1.0.conv1.stats tensor(29.6261, grad_fn=<MulBackward0>)\n",
            "layer1.0.conv2.stats tensor(28.6099, grad_fn=<MulBackward0>)\n",
            "layer1.0.add_relu.stats tensor(32.5803, grad_fn=<MulBackward0>)\n",
            "layer1.1.conv1.stats tensor(29.2803, grad_fn=<MulBackward0>)\n",
            "layer1.1.conv2.stats tensor(25.5205, grad_fn=<MulBackward0>)\n",
            "layer1.1.add_relu.stats tensor(29.2830, grad_fn=<MulBackward0>)\n",
            "layer2.0.conv1.stats tensor(26.3501, grad_fn=<MulBackward0>)\n",
            "layer2.0.conv2.stats tensor(25.4438, grad_fn=<MulBackward0>)\n",
            "layer2.0.downsample.0.stats tensor(22.4520, grad_fn=<MulBackward0>)\n",
            "layer2.0.add_relu.stats tensor(25.4528, grad_fn=<MulBackward0>)\n",
            "layer2.1.conv1.stats tensor(24.6089, grad_fn=<MulBackward0>)\n",
            "layer2.1.conv2.stats tensor(24.0096, grad_fn=<MulBackward0>)\n",
            "layer2.1.add_relu.stats tensor(25.7042, grad_fn=<MulBackward0>)\n",
            "layer3.0.conv1.stats tensor(26.7143, grad_fn=<MulBackward0>)\n",
            "layer3.0.conv2.stats tensor(27.0136, grad_fn=<MulBackward0>)\n",
            "layer3.0.downsample.0.stats tensor(25.4694, grad_fn=<MulBackward0>)\n",
            "layer3.0.add_relu.stats tensor(25.2878, grad_fn=<MulBackward0>)\n",
            "layer3.1.conv1.stats tensor(31.3015, grad_fn=<MulBackward0>)\n",
            "layer3.1.conv2.stats tensor(25.8686, grad_fn=<MulBackward0>)\n",
            "layer3.1.add_relu.stats tensor(25.3074, grad_fn=<MulBackward0>)\n",
            "layer4.0.conv1.stats tensor(27.1386, grad_fn=<MulBackward0>)\n",
            "layer4.0.conv2.stats tensor(27.1376, grad_fn=<MulBackward0>)\n",
            "layer4.0.downsample.0.stats tensor(21.8550, grad_fn=<MulBackward0>)\n",
            "layer4.0.add_relu.stats tensor(20.2886, grad_fn=<MulBackward0>)\n",
            "layer4.1.conv1.stats tensor(25.2966, grad_fn=<MulBackward0>)\n",
            "layer4.1.conv2.stats tensor(17.6837, grad_fn=<MulBackward0>)\n",
            "layer4.1.add_relu.stats tensor(18.2189, grad_fn=<MulBackward0>)\n",
            "fc.stats tensor(20.3691, grad_fn=<MulBackward0>)\n",
            "quant.stats tensor(47.7226)\n"
          ]
        }
      ],
      "source": [
        "def compute_error(x, y):\n",
        "    Ps = torch.norm(x)\n",
        "    Pn = torch.norm(x - y)\n",
        "    return 20 * torch.log10(Ps/Pn)\n",
        "\n",
        "wt_compare_dict = ns.compare_weights(float_model.state_dict(), qmodel.state_dict())\n",
        "for key in wt_compare_dict:\n",
        "    print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "act_compare_dict = ns.compare_model_outputs(float_model, qmodel, img_data[0])\n",
        "for key in act_compare_dict:\n",
        "    print(key, compute_error(act_compare_dict[key]['float'][0], act_compare_dict[key]['quantized'][0].dequantize()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "eYUaWoAyeL7a",
        "outputId": "4444d220-b100-422b-f0db-3a63daa25f9e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f = wt_compare_dict['conv1.weight']['float'].flatten()\n",
        "plt.hist(f, bins = 100)\n",
        "\n",
        "q = wt_compare_dict['conv1.weight']['quantized'].flatten().dequantize()\n",
        "plt.hist(q, bins = 100)\n",
        "plt.title(\"Quantized model weights of conv1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YQ7S0iIb8cz"
      },
      "source": [
        "### RECOMMENDATIONS FOR YOUR WORKFLOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-cxT3iwb9jU"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ij7GElwiKFNmJ221aKpkDRwMj5i-2DLh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBbyE61acqr6"
      },
      "source": [
        "\n",
        "\n",
        "*   Large (10M+ parameters) models are more robust to quantization error\n",
        "*   Quantizing a model from a FP32 checkpoint provides better accuracy than training an INT8 model from scratch\n",
        "*   Dynamic Quantization is an easy first step, especially if your model has many Linear or Recurrent layers\n",
        "*   Use symmetric-per-channel quantization with MinMax observers for quantizing weights. Use affine-per-tensor quantization with MovingAverageMinMax observers for quantizing activations\n",
        "*   Use metrics like SQNR to identify which layers are most suscpetible to quantization error. Turn off quantization on these layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZkMfDNCp9f1"
      },
      "source": [
        "Eager mode https://pytorch.org/docs/stable/quantization.html#eager-mode-quantization\n",
        "\n",
        "\n",
        "\n",
        "*   https://pytorch.org/docs/stable/quantization.html#post-training-dynamic-quantization\n",
        "*   https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization\n",
        "\n",
        "\n",
        "FX Graph https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization\n",
        "\n",
        "\n",
        "\n",
        "*   https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html\n",
        "*   https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za9ziysYh-7V"
      },
      "source": [
        "## New forkflow with torchao\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHc-7SBJiWid",
        "outputId": "d14e6adf-5c58-4299-df95-08e175ba1aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (0.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchao"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LP_lwV2iGXF"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "\n",
        "class ToyLinearModel(torch.nn.Module):\n",
        "    def __init__(self, m: int, n: int, k: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(m, n, bias=False)\n",
        "        self.linear2 = torch.nn.Linear(n, k, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "model = ToyLinearModel(1024, 1024, 1024).eval()\n",
        "\n",
        "# Optional: compile model for faster inference and generation\n",
        "model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n",
        "model_f32 = copy.deepcopy(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cYxfwkt-xrX"
      },
      "outputs": [],
      "source": [
        "from torchao.quantization import Int4DynamicActivationInt4WeightConfig, quantize_\n",
        "quantize_(model, Int4DynamicActivationInt4WeightConfig())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbGjMaxGj20y",
        "outputId": "5b204369-6052-439c-c4bd-cb6c3378862b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f32 mean time: 0.237 ms\n",
            "int4 mean time: 0.076 ms\n",
            "speedup: 3.1x\n"
          ]
        }
      ],
      "source": [
        "from torchao.utils import (\n",
        "    benchmark_model,\n",
        "    unwrap_tensor_subclass,\n",
        ")\n",
        "\n",
        "num_runs = 100\n",
        "torch._dynamo.reset()\n",
        "example_inputs = (torch.randn(1, 1024, dtype=torch.float32),)\n",
        "f32_time = benchmark_model(model_f32, num_runs, example_inputs)\n",
        "int4_time = benchmark_model(model, num_runs, example_inputs)\n",
        "\n",
        "print(\"f32 mean time: %0.3f ms\" % f32_time)\n",
        "print(\"int4 mean time: %0.3f ms\" % int4_time)\n",
        "print(\"speedup: %0.1fx\" % (f32_time / int4_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWDUskVC-H1z",
        "outputId": "f7ab2c21-5c79-4bf3-839a-dc2bc7ee9684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "int4 model size: 1.01 MB\n",
            "f32 model size: 8.00 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "torch.save(model, \"/tmp/int4_model.pt\")\n",
        "torch.save(model_f32, \"/tmp/f32_model.pt\")\n",
        "int4_model_size_mb = os.path.getsize(\"/tmp/int4_model.pt\") / 1024 / 1024\n",
        "f32_model_size_mb = os.path.getsize(\"/tmp/f32_model.pt\") / 1024 / 1024\n",
        "\n",
        "print(\"int4 model size: %.2f MB\" % int4_model_size_mb)\n",
        "\n",
        "print(\"f32 model size: %.2f MB\" % f32_model_size_mb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI8kSIybQcEQ"
      },
      "source": [
        "### Neural Network Compression Framework (NNCF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7q1Gh6YQfhu",
        "outputId": "de9fb633-1d8a-46a8-c7d6-8a03693b56af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nncf'...\n",
            "remote: Enumerating objects: 89813, done.\u001b[K\n",
            "remote: Counting objects: 100% (1530/1530), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1043/1043), done.\u001b[K\n",
            "remote: Total 89813 (delta 1061), reused 523 (delta 483), pack-reused 88283 (from 4)\u001b[K\n",
            "Receiving objects: 100% (89813/89813), 67.76 MiB | 23.91 MiB/s, done.\n",
            "Resolving deltas: 100% (60290/60290), done.\n",
            "Filtering content: 100% (149/149), 32.33 MiB | 12.19 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/openvinotoolkit/nncf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7inenttGuOti"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "%cd nncf\n",
        "!pip install .[torch]\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwLu99qZ3IEl",
        "outputId": "5f77819a-5692-4c92-a289-2106505ac778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:nncf:NNCF provides best results with torch==2.9.*, while current torch version is 2.8.0+cu126. If you encounter issues, consider switching to torch==2.9.*\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import nncf.torch  # Important - must be imported before any other external package that depends on torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwXZ_Te7El1C"
      },
      "source": [
        "https://github.com/openvinotoolkit/nncf/tree/develop\n",
        "https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md\n",
        "https://github.com/openvinotoolkit/nncf/blob/develop/examples/torch/classification/configs/quantization/inception_v3_imagenet_int8.json\n",
        "\n",
        "https://dev-discuss.pytorch.org/t/torch-ao-quantization-migration-plan/2810\n",
        "https://docs.pytorch.org/ao/stable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysq3_CdFoNuu"
      },
      "source": [
        "** **:  Post Training Quantization   float32 torch2     .        .          (.  ).         SENSITIVITY .      .     / ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhg-qYqfWP5K",
        "outputId": "7d83b64a-8089-49f3-820f-fb6d8cf03673"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 170M/170M [00:14<00:00, 12.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 44.7M/44.7M [00:00<00:00, 120MB/s]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "import torch.ao.quantization as tq\n",
        "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "from torch.ao.quantization.observer import HistogramObserver, PerChannelMinMaxObserver\n",
        "\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "\n",
        "backend = \"fbgemm\"\n",
        "torch.backends.quantized.engine = backend\n",
        "device_eval = torch.device(\"cpu\")\n",
        "\n",
        "data_root = \"./data\"\n",
        "calib_subset_size = 1024\n",
        "test_subset_size = 5000\n",
        "batch_size_calib = 64\n",
        "batch_size_test = 128\n",
        "num_calib_batches = 50\n",
        "num_latency_batches = 50\n",
        "\n",
        "\n",
        "#  CIFAR-10\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=data_root, train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=data_root, train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "calib_dataset, _ = random_split(\n",
        "    train_dataset,\n",
        "    [calib_subset_size, len(train_dataset) - calib_subset_size],\n",
        ")\n",
        "\n",
        "test_dataset_eval, _ = random_split(\n",
        "    test_dataset,\n",
        "    [test_subset_size, len(test_dataset) - test_subset_size],\n",
        ")\n",
        "\n",
        "calib_loader = DataLoader(\n",
        "    calib_dataset, batch_size=batch_size_calib, shuffle=True,\n",
        "    num_workers=2, pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset_eval, batch_size=batch_size_test, shuffle=False,\n",
        "    num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "#  ResNet18 (float32 baseline)\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, 10)\n",
        "model.to(\"cuda\")\n",
        "model.train()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "model.to(\"cpu\")\n",
        "float_model = model\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_accuracy(model: nn.Module,\n",
        "                      dataloader: DataLoader,\n",
        "                      device: torch.device) -> float:\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_latency_ms(model: nn.Module,\n",
        "                       dataloader: DataLoader,\n",
        "                       device: torch.device,\n",
        "                       num_batches: int = 50) -> float:\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    it = iter(dataloader)\n",
        "\n",
        "    warmup_batches = min(10, num_batches)\n",
        "    for _ in range(warmup_batches):\n",
        "        try:\n",
        "            images, _ = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(dataloader)\n",
        "            images, _ = next(it)\n",
        "        images = images.to(device)\n",
        "        _ = model(images)\n",
        "\n",
        "    times = []\n",
        "    it = iter(dataloader)\n",
        "    for _ in range(num_batches):\n",
        "        try:\n",
        "            images, _ = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(dataloader)\n",
        "            images, _ = next(it)\n",
        "        images = images.to(device)\n",
        "\n",
        "        start = time.perf_counter()\n",
        "        _ = model(images)\n",
        "        end = time.perf_counter()\n",
        "        times.append(end - start)\n",
        "\n",
        "    return (sum(times) / len(times)) * 1000.0\n",
        "\n",
        "\n",
        "#   PTQ (FX-Mode)\n",
        "\n",
        "def ptq_fx_full(model_fp32: nn.Module,\n",
        "                calib_loader: DataLoader,\n",
        "                qconfig,\n",
        "                num_calib_batches: int = 50) -> nn.Module:\n",
        "    \"\"\"\n",
        "      :\n",
        "    -  \n",
        "    -  (prepare_fx)\n",
        "    -   \n",
        "    -  (convert_fx)\n",
        "    \"\"\"\n",
        "    model = copy.deepcopy(model_fp32).cpu().eval()\n",
        "\n",
        "    example_inputs = (next(iter(calib_loader))[0],)\n",
        "    qconfig_dict = {\"\": qconfig}\n",
        "\n",
        "    prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batches_seen = 0\n",
        "        for images, _ in calib_loader:\n",
        "            prepared(images)\n",
        "            batches_seen += 1\n",
        "            if batches_seen >= num_calib_batches:\n",
        "                break\n",
        "\n",
        "    quantized = convert_fx(prepared)\n",
        "    return quantized\n",
        "\n",
        "\n",
        "\n",
        "def sensitivity_analysis(model_fp32: nn.Module,\n",
        "                         calib_loader: DataLoader,\n",
        "                         test_loader: DataLoader,\n",
        "                         baseline_acc: float,\n",
        "                         backend: str = \"fbgemm\",\n",
        "                         num_calib_batches: int = 30,\n",
        "                         max_layers: int = 10):\n",
        "\n",
        "    results = []\n",
        "    default_qconfig = tq.get_default_qconfig(backend)\n",
        "\n",
        "    candidate_layers = [\n",
        "        name for name, module in model_fp32.named_modules()\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear))\n",
        "    ]\n",
        "\n",
        "    candidate_layers = candidate_layers[:max_layers]\n",
        "\n",
        "    for name in candidate_layers:\n",
        "        print(f\"[*] Sensitivity: quantizing only layer {name}\")\n",
        "\n",
        "        model = copy.deepcopy(model_fp32).cpu().eval()\n",
        "        example_inputs = (next(iter(calib_loader))[0],)\n",
        "\n",
        "        qconfig_dict = {\n",
        "            \"\": None,\n",
        "            \"module_name\": [(name, default_qconfig)],\n",
        "        }\n",
        "\n",
        "        prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batches_seen = 0\n",
        "            for images, _ in calib_loader:\n",
        "                prepared(images)\n",
        "                batches_seen += 1\n",
        "                if batches_seen >= num_calib_batches:\n",
        "                    break\n",
        "\n",
        "        quantized = convert_fx(prepared)\n",
        "        acc = evaluate_accuracy(quantized, test_loader, device_eval)\n",
        "        drop = baseline_acc - acc\n",
        "        print(f\"    acc={acc:.4f}, drop={drop:.4f}\")\n",
        "\n",
        "        results.append({\"layer\": name, \"acc\": acc, \"drop\": drop})\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def ptq_fx_with_skipped_layers(model_fp32: nn.Module,\n",
        "                               calib_loader: DataLoader,\n",
        "                               skip_layer_names,\n",
        "                               backend: str = \"fbgemm\",\n",
        "                               num_calib_batches: int = 50) -> nn.Module:\n",
        "    \"\"\"\n",
        "     ,    skip_layer_names   FP32\n",
        "     qconfig_dict (  qconfig=None).\n",
        "    \"\"\"\n",
        "    model = copy.deepcopy(model_fp32).cpu().eval()\n",
        "    example_inputs = (next(iter(calib_loader))[0],)\n",
        "\n",
        "    default_qconfig = tq.get_default_qconfig(backend)\n",
        "    module_name_qconfigs = [(name, None) for name in skip_layer_names]\n",
        "\n",
        "    qconfig_dict = {\n",
        "        \"\": default_qconfig,\n",
        "        \"module_name\": module_name_qconfigs,\n",
        "    }\n",
        "\n",
        "    prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batches_seen = 0\n",
        "        for images, _ in calib_loader:\n",
        "            prepared(images)\n",
        "            batches_seen += 1\n",
        "            if batches_seen >= num_calib_batches:\n",
        "                break\n",
        "\n",
        "    quantized = convert_fx(prepared)\n",
        "    return quantized\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPWvCg1-WV4f",
        "outputId": "0d7293f6-a837-4a61-85dd-d9769f49b89f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Baseline FP32 ===\n",
            "Baseline acc=0.8000, latency=11449.37 ms/batch\n",
            "\n",
            "=== PTQ: default qconfig ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:198: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:208: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PTQ default acc=0.7584, latency=5365.53 ms/batch\n",
            "\n",
            "=== PTQ: HistogramObserver + per-channel weights ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:198: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/tmp/ipython-input-2561546971.py:208: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PTQ hist acc=0.6936, latency=5303.50 ms/batch\n",
            "\n",
            "=== Sensitivity analysis (one layer at a time) ===\n",
            "[*] Sensitivity: quantizing only layer conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.7680, drop=0.0320\n",
            "[*] Sensitivity: quantizing only layer layer1.0.conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.7928, drop=0.0072\n",
            "[*] Sensitivity: quantizing only layer layer1.0.conv2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8024, drop=-0.0024\n",
            "[*] Sensitivity: quantizing only layer layer1.1.conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8002, drop=-0.0002\n",
            "[*] Sensitivity: quantizing only layer layer1.1.conv2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.7998, drop=0.0002\n",
            "[*] Sensitivity: quantizing only layer layer2.0.conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8006, drop=-0.0006\n",
            "[*] Sensitivity: quantizing only layer layer2.0.conv2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8016, drop=-0.0016\n",
            "[*] Sensitivity: quantizing only layer layer2.0.downsample.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8008, drop=-0.0008\n",
            "[*] Sensitivity: quantizing only layer layer2.1.conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.7996, drop=0.0004\n",
            "[*] Sensitivity: quantizing only layer layer2.1.conv2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:263: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8002, drop=-0.0002\n",
            "\n",
            "Top sensitive layers (  accuracy):\n",
            "                   layer     acc    drop\n",
            "0                  conv1  0.7680  0.0320\n",
            "1         layer1.0.conv1  0.7928  0.0072\n",
            "8         layer2.1.conv1  0.7996  0.0004\n",
            "4         layer1.1.conv2  0.7998  0.0002\n",
            "9         layer2.1.conv2  0.8002 -0.0002\n",
            "3         layer1.1.conv1  0.8002 -0.0002\n",
            "5         layer2.0.conv1  0.8006 -0.0006\n",
            "7  layer2.0.downsample.0  0.8008 -0.0008\n",
            "6         layer2.0.conv2  0.8016 -0.0016\n",
            "2         layer1.0.conv2  0.8024 -0.0024\n",
            "\n",
            "  (  FP32) : ['conv1', 'layer1.0.conv1', 'layer2.1.conv1']\n",
            "\n",
            "=== PTQ with sensitivity (skip sensitive layers) ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2561546971.py:293: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2561546971.py:303: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PTQ (skip sensitive) acc=0.7850, latency=7014.39 ms/batch\n",
            "\n",
            "=== Summary: accuracy / latency ===\n",
            "                                variant        type  sensitivity_used     acc  \\\n",
            "0                         FP32 baseline        FP32             False  0.8000   \n",
            "1                   PTQ default_qconfig  static PTQ             False  0.7584   \n",
            "2                 PTQ HistogramObserver  static PTQ             False  0.6936   \n",
            "3  PTQ default_qconfig + skip sensitive  static PTQ              True  0.7850   \n",
            "\n",
            "   latency_ms_per_batch  \n",
            "0          11449.373756  \n",
            "1           5365.531203  \n",
            "2           5303.499538  \n",
            "3           7014.386685  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "results = []\n",
        "\n",
        "# Float32 baseline\n",
        "print(\"=== Baseline FP32 ===\")\n",
        "baseline_acc = evaluate_accuracy(float_model, test_loader, device_eval)\n",
        "baseline_latency = measure_latency_ms(float_model, test_loader, device_eval,\n",
        "                                      num_batches=num_latency_batches)\n",
        "\n",
        "results.append({\n",
        "    \"variant\": \"FP32 baseline\",\n",
        "    \"type\": \"FP32\",\n",
        "    \"sensitivity_used\": False,\n",
        "    \"acc\": baseline_acc,\n",
        "    \"latency_ms_per_batch\": baseline_latency,\n",
        "})\n",
        "\n",
        "print(f\"Baseline acc={baseline_acc:.4f}, latency={baseline_latency:.2f} ms/batch\")\n",
        "\n",
        "# PTQ: default qconfig (MinMax / per-tensor / per-channel    backend)\n",
        "print(\"\\n=== PTQ: default qconfig ===\")\n",
        "default_qconfig = tq.get_default_qconfig(backend)\n",
        "model_ptq_default = ptq_fx_full(\n",
        "    float_model, calib_loader,\n",
        "    qconfig=default_qconfig,\n",
        "    num_calib_batches=num_calib_batches\n",
        ")\n",
        "\n",
        "ptq_def_acc = evaluate_accuracy(model_ptq_default, test_loader, device_eval)\n",
        "ptq_def_latency = measure_latency_ms(model_ptq_default, test_loader, device_eval,\n",
        "                                     num_batches=num_latency_batches)\n",
        "\n",
        "results.append({\n",
        "    \"variant\": \"PTQ default_qconfig\",\n",
        "    \"type\": \"static PTQ\",\n",
        "    \"sensitivity_used\": False,\n",
        "    \"acc\": ptq_def_acc,\n",
        "    \"latency_ms_per_batch\": ptq_def_latency,\n",
        "})\n",
        "\n",
        "print(f\"PTQ default acc={ptq_def_acc:.4f}, latency={ptq_def_latency:.2f} ms/batch\")\n",
        "\n",
        "# PTQ:  qconfig  HistogramObserver  \n",
        "print(\"\\n=== PTQ: HistogramObserver + per-channel weights ===\")\n",
        "hist_qconfig = tq.QConfig(\n",
        "    activation=HistogramObserver.with_args(\n",
        "        dtype=torch.quint8,\n",
        "        qscheme=torch.per_tensor_affine\n",
        "    ),\n",
        "    weight=PerChannelMinMaxObserver.with_args(\n",
        "        dtype=torch.qint8,\n",
        "        qscheme=torch.per_channel_symmetric\n",
        "    ),\n",
        ")\n",
        "\n",
        "model_ptq_hist = ptq_fx_full(\n",
        "    float_model, calib_loader,\n",
        "    qconfig=hist_qconfig,\n",
        "    num_calib_batches=num_calib_batches\n",
        ")\n",
        "\n",
        "ptq_hist_acc = evaluate_accuracy(model_ptq_hist, test_loader, device_eval)\n",
        "ptq_hist_latency = measure_latency_ms(model_ptq_hist, test_loader, device_eval,\n",
        "                                      num_batches=num_latency_batches)\n",
        "\n",
        "results.append({\n",
        "    \"variant\": \"PTQ HistogramObserver\",\n",
        "    \"type\": \"static PTQ\",\n",
        "    \"sensitivity_used\": False,\n",
        "    \"acc\": ptq_hist_acc,\n",
        "    \"latency_ms_per_batch\": ptq_hist_latency,\n",
        "})\n",
        "\n",
        "print(f\"PTQ hist acc={ptq_hist_acc:.4f}, latency={ptq_hist_latency:.2f} ms/batch\")\n",
        "\n",
        "print(\"\\n=== Sensitivity analysis (one layer at a time) ===\")\n",
        "sens_results = sensitivity_analysis(\n",
        "    float_model,\n",
        "    calib_loader,\n",
        "    test_loader,\n",
        "    baseline_acc=baseline_acc,\n",
        "    backend=backend,\n",
        "    num_calib_batches=30,\n",
        "    max_layers=10,\n",
        ")\n",
        "\n",
        "sens_df = pd.DataFrame(sens_results).sort_values(\"drop\", ascending=False)\n",
        "print(\"\\nTop sensitive layers (  accuracy):\")\n",
        "print(sens_df)\n",
        "\n",
        "num_skip = min(3, len(sens_df))\n",
        "skip_layers = sens_df.head(num_skip)[\"layer\"].tolist()\n",
        "print(f\"\\n  (  FP32) : {skip_layers}\")\n",
        "\n",
        "print(\"\\n=== PTQ with sensitivity (skip sensitive layers) ===\")\n",
        "model_ptq_sens = ptq_fx_with_skipped_layers(\n",
        "    float_model,\n",
        "    calib_loader,\n",
        "    skip_layer_names=skip_layers,\n",
        "    backend=backend,\n",
        "    num_calib_batches=num_calib_batches\n",
        ")\n",
        "\n",
        "ptq_sens_acc = evaluate_accuracy(model_ptq_sens, test_loader, device_eval)\n",
        "ptq_sens_latency = measure_latency_ms(model_ptq_sens, test_loader, device_eval,\n",
        "                                      num_batches=num_latency_batches)\n",
        "\n",
        "results.append({\n",
        "    \"variant\": \"PTQ default_qconfig + skip sensitive\",\n",
        "    \"type\": \"static PTQ\",\n",
        "    \"sensitivity_used\": True,\n",
        "    \"acc\": ptq_sens_acc,\n",
        "    \"latency_ms_per_batch\": ptq_sens_latency,\n",
        "})\n",
        "\n",
        "print(f\"PTQ (skip sensitive) acc={ptq_sens_acc:.4f}, latency={ptq_sens_latency:.2f} ms/batch\")\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== Summary: accuracy / latency ===\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3ll9vbq-UU5",
        "outputId": "0d8afc06-1bd5-4818-d271-7eb2da2ae9c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary: accuracy / latency ===\n",
            "                                variant        type  sensitivity_used     acc  \\\n",
            "0                         FP32 baseline        FP32             False  0.8000   \n",
            "1                   PTQ default_qconfig  static PTQ             False  0.7584   \n",
            "2                 PTQ HistogramObserver  static PTQ             False  0.6936   \n",
            "3  PTQ default_qconfig + skip sensitive  static PTQ              True  0.7850   \n",
            "\n",
            "   latency_ms_per_batch  \n",
            "0          11449.373756  \n",
            "1           5365.531203  \n",
            "2           5303.499538  \n",
            "3           7014.386685  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n=== Summary: accuracy / latency ===\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_7cOvdWLuSa"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
